{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "random_seed = 1234\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '1, 2' # '0'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda')\n",
    "print(device)\n",
    "\n",
    "final_dict = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = torch.load('data/Training/X_train_diet.pt') # torch.Size([16008, 240, 320, 1]) # X_train_diet.pt # X_train.pt\n",
    "X_test_ = torch.load('data/Testing/X_test_diet.pt') # torch.Size([1596, 240, 320, 1]) # X_test_diet.pt # X_test.pt\n",
    "\n",
    "Y_train_ = pd.read_csv('data/Training/Y_train.csv')\n",
    "Y_test_ = pd.read_csv('data/Testing/Y_test.csv')\n",
    "# dataframe to tensor\n",
    "Y_train_ = torch.tensor(Y_train_.values)  # torch.Size([16008, 63])\n",
    "Y_test_ = torch.tensor(Y_test_.values)  # torch.Size([1596, 63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = X_train_.float()\n",
    "X_test_ = X_test_.float()\n",
    "\n",
    "mean = 1881.42\n",
    "std = 12.29\n",
    "\n",
    "# Standardise\n",
    "X_train_ -= mean\n",
    "X_train_ /= std\n",
    "X_test_ -= mean\n",
    "X_test_ /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1596, 1, 240, 320])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# permute data\n",
    "\n",
    "X_train_ = X_train_.permute([0, 3, 1, 2])\n",
    "X_train_.shape\n",
    "\n",
    "X_test_ = X_test_.permute([0, 3, 1, 2])\n",
    "X_test_.shape\n",
    "\n",
    "# torch.Size([1596, 1, 240, 320])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_split_dataloader(X_train_, X_test_, Y_train_, Y_test_, sample_size, valid_size, batch_size):\n",
    "\n",
    "    # sample train set\n",
    "    n_train = len(X_train_)\n",
    "    indices = list(range(n_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(sample_size * n_train))\n",
    "    sample_indices = indices[:split]\n",
    "    X_train_ = X_train_[sample_indices]\n",
    "    Y_train_ = Y_train_[sample_indices]\n",
    "\n",
    "    # # sample test set\n",
    "    # n_test = len(X_test_)\n",
    "    # indices = list(range(n_test))\n",
    "    # np.random.shuffle(indices)\n",
    "    # split = int(np.floor(sample_size * n_test))\n",
    "    # sample_indices = indices[:split]\n",
    "    # X_test_ = X_test_[sample_indices]\n",
    "    # Y_test_ = Y_test_[sample_indices]\n",
    "\n",
    "    # split train, valid set\n",
    "    n_train = len(X_train_)                  \n",
    "    indices = list(range(n_train))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split = int(np.floor(valid_size * n_train)) \n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    train_set = TensorDataset(X_train_, Y_train_)\n",
    "    train_sampler, valid_sampler = SubsetRandomSampler(train_idx), SubsetRandomSampler(valid_idx) \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=train_sampler)\n",
    "    valid_loader = DataLoader(train_set, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    # test loader\n",
    "    test_set = TensorDataset(X_test_, Y_test_)\n",
    "    test_loader = DataLoader(test_set, batch_size=1)\n",
    "\n",
    "    # print shape\n",
    "    print('\\n Sample size: ', sample_size)\n",
    "    print('train_loader: ', len(train_loader)*train_loader.batch_size )\n",
    "    print('valid_loader: ', len(valid_loader)*valid_loader.batch_size )\n",
    "    print('test_loader: ', len(test_loader)*test_loader.batch_size )\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_CNN(nn.Module): \n",
    "\n",
    "    def __init__(self, n_class, CHANNELS, KERNEL_SIZE, STRIDE, PADDING):\n",
    "        # super(my_CNN, self).__init__()\n",
    "        super().__init__()\n",
    "        self.n_class = n_class\n",
    "        self.cs = CHANNELS\n",
    "        self.KERNEL_SIZE = KERNEL_SIZE\n",
    "        self.STRIDE = STRIDE\n",
    "        self.PADDING = PADDING\n",
    "        # self.dropout - nn.Dropout(0.4)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, self.cs[0], kernel_size=KERNEL_SIZE, stride=STRIDE, padding=PADDING), # 240 * 320 * self.cs[0]\n",
    "            nn.BatchNorm2d(self.cs[0]),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2) # 120 * 160 * self.cs[0]\n",
    "        ) \n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(self.cs[0], self.cs[1], kernel_size=KERNEL_SIZE, stride=STRIDE, padding=PADDING), # 120 * 160 * self.cs[1]\n",
    "            nn.BatchNorm2d(self.cs[1]),\n",
    "            nn.ReLU()\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2) # 60 * 80 * self.cs[1]\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(self.cs[1], self.cs[2], kernel_size=KERNEL_SIZE, stride=STRIDE, padding=PADDING), # 120 * 160 * self.cs[2]\n",
    "            nn.BatchNorm2d(self.cs[2]),\n",
    "            nn.ReLU(),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2)  # 60 * 80 * self.cs[2]\n",
    "        )\n",
    "        \n",
    "        self.fc = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        out1 = self.layer1(x) \n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out4 = out3.reshape(out3.size(0), -1) \n",
    "        ###\n",
    "        shape1, shape2 = out3.shape[2], out3.shape[3]\n",
    "        self.fc = nn.Linear(shape1 * shape2 * self.cs[2], self.n_class).to(device)\n",
    "        ###\n",
    "        out4 = out4.to(device)\n",
    "        out = self.fc(out4)  #######################################################\n",
    "\n",
    "        # print(x.shape)  # torch.Size([1, 1, 240, 320])\n",
    "        # print(out1.shape)  # torch.Size([1, 16, 120, 160])\n",
    "        # print(out2.shape)  # torch.Size([1, 32, 120, 160])\n",
    "        # print(out3.shape)  # torch.Size([1, 64, 60, 80])\n",
    "        # print(out4.shape) # torch.Size([1, 307200])\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# MatMul shape 맞는지 체크\n",
    "\n",
    "# a = my_CNN(n_class=63, KERNEL_SIZE=KERNEL_SIZE, STRIDE=STRIDE, PADDING=PADDING)\n",
    "# a(torch.randn(1, 1, 240, 320))  # B C H W 로 가짜 데이터 넣어보고 체크\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = LR)\n",
    "    criterion = nn.MSELoss()\n",
    "    valid_loss_min = np.inf # 초기화 (나중에 업데이트 함)\n",
    "    records = {}\n",
    "\n",
    "    for epoch in tqdm(range(1, Train_epoch + 1)):\n",
    "\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for batch_id, (image, label) in enumerate(train_loader): # iter: batch 데이터 (25개) \n",
    "\n",
    "            label, image = label.to(device), image.float().to(device) # shape: (25,)\n",
    "            output = model(image)   # 1. 모델에 데이터 입력해 출력 얻기 # 10개 클래스에 대한 로짓 # shape: (25, 10)\n",
    "            loss = criterion(output.float(), label.float()) # 2. loss 계산 \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad() # 3. 기울기 초기화 (iter 끝날때마다 초기화)\n",
    "            loss.backward() # 4. 역전파\n",
    "            optimizer.step() # 5. 최적화\n",
    "        \n",
    "        for batch_id, (image, label) in enumerate(valid_loader):\n",
    "\n",
    "            label, image = label.to(device), image.float().to(device)\n",
    "            output = model(image)\n",
    "            loss = criterion(output.float(), label.float())\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "        # calculate avg losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "\n",
    "        # print training/validation records \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "        # save training/validation records \n",
    "        records[f'epoch_{epoch}'] = [train_loss, valid_loss]\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "            torch.save(\n",
    "                model, \n",
    "                f'./records/CNN_best_model_kernel_{model.KERNEL_SIZE}_stride_{model.STRIDE}.pt')\n",
    "            torch.save(\n",
    "                model.state_dict(), \n",
    "                f'./records/CNN_best_model_kernel_{model.KERNEL_SIZE}_stride_{model.STRIDE}.pth')\n",
    "            valid_loss_min = valid_loss\n",
    "    # save records\n",
    "    pd.DataFrame(records).to_csv(\n",
    "        f'./records/CNN_training_kernel_{model.KERNEL_SIZE}_stride_{model.STRIDE}.csv', index=False)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def test(model):\n",
    "\n",
    "    print('success load best_model')\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():  # 파라미터 업데이트 안 함\n",
    "\n",
    "        for batch_id, (image, label) in enumerate(tqdm(test_loader)):\n",
    "\n",
    "            label, image = label.to(device), image.float().to(device)\n",
    "            output = model(image)\n",
    "            loss = criterion(output.float(), label.float())\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    # calculate avg losses\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training parameters\n",
    "\n",
    "n_class = 63\n",
    "Train_epoch = 30\n",
    "BATCH_SIZE = 128\n",
    "LR = 5e-2\n",
    "\n",
    "# ablation studies\n",
    "\n",
    "batch_norm = None\n",
    "activation = None\n",
    "max_pooling = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_runtime_dict = {}\n",
    "\n",
    "# hyperparameters\n",
    "param_grid = {\n",
    "    'CHANNELS': [[3, 6, 9]],\n",
    "    'KERNEL_SIZE' : [3, 6],                  ###\n",
    "    'STRIDE' : [1, 5],                       ###\n",
    "    'PADDING' : [0]                          \n",
    "}\n",
    "\n",
    "grid_list = list(ParameterGrid(param_grid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample size:  0.3\n",
      "train_loader:  3968\n",
      "valid_loader:  1024\n",
      "test_loader:  1596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.000899 \tValidation Loss: 0.000170\n",
      "Validation loss decreased (inf --> 0.000170).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tTraining Loss: 0.000658 \tValidation Loss: 0.000150\n",
      "Validation loss decreased (0.000170 --> 0.000150).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tTraining Loss: 0.000556 \tValidation Loss: 0.000144\n",
      "Validation loss decreased (0.000150 --> 0.000144).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tTraining Loss: 0.000507 \tValidation Loss: 0.000120\n",
      "Validation loss decreased (0.000144 --> 0.000120).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tTraining Loss: 0.000443 \tValidation Loss: 0.000120\n",
      "Epoch: 6 \tTraining Loss: 0.000422 \tValidation Loss: 0.000099\n",
      "Validation loss decreased (0.000120 --> 0.000099).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tTraining Loss: 0.000383 \tValidation Loss: 0.000094\n",
      "Validation loss decreased (0.000099 --> 0.000094).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 \tTraining Loss: 0.000352 \tValidation Loss: 0.000088\n",
      "Validation loss decreased (0.000094 --> 0.000088).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 \tTraining Loss: 0.000339 \tValidation Loss: 0.000087\n",
      "Validation loss decreased (0.000088 --> 0.000087).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 \tTraining Loss: 0.000321 \tValidation Loss: 0.000083\n",
      "Validation loss decreased (0.000087 --> 0.000083).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 \tTraining Loss: 0.000308 \tValidation Loss: 0.000080\n",
      "Validation loss decreased (0.000083 --> 0.000080).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 \tTraining Loss: 0.000299 \tValidation Loss: 0.000078\n",
      "Validation loss decreased (0.000080 --> 0.000078).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 \tTraining Loss: 0.000297 \tValidation Loss: 0.000074\n",
      "Validation loss decreased (0.000078 --> 0.000074).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 \tTraining Loss: 0.000279 \tValidation Loss: 0.000071\n",
      "Validation loss decreased (0.000074 --> 0.000071).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 \tTraining Loss: 0.000283 \tValidation Loss: 0.000070\n",
      "Validation loss decreased (0.000071 --> 0.000070).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 \tTraining Loss: 0.000274 \tValidation Loss: 0.000072\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    sample_size_list = [0.3, 0.6, 1.0]\n",
    "\n",
    "    for sample_size in tqdm(sample_size_list):\n",
    "\n",
    "        train_loader, valid_loader, test_loader = sample_and_split_dataloader(\n",
    "            X_train_, X_test_, Y_train_, Y_test_, sample_size=sample_size, valid_size=0.2, batch_size=BATCH_SIZE)\n",
    "\n",
    "\n",
    "        for case in tqdm(grid_list):\n",
    "\n",
    "            # try:\n",
    "            regr = my_CNN(n_class=63, CHANNELS=case['CHANNELS'], KERNEL_SIZE=case['KERNEL_SIZE'],\n",
    "                          STRIDE=case['STRIDE'], PADDING=case['PADDING']\n",
    "                    ).to(device)\n",
    "\n",
    "            start_time = datetime.now()\n",
    "            model_trained = train(regr)\n",
    "            train_time = datetime.now() - start_time\n",
    "\n",
    "            acc_runtime_dict[str(case)] = dict({'train_time': train_time})\n",
    "            # except:\n",
    "            #     pass\n",
    "\n",
    "        final_dict[sample_size] = acc_runtime_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open('result_CNN.pickle', 'wb') as f:\n",
    "    pickle.dump(final_dict, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# load\n",
    "with open('result_CNN.pickle', 'rb') as f:\n",
    "    result_RF = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test는 따로 best model을 torch.load로 가져와서 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success load best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1596/1596 [07:43<00:00,  3.45it/s]\n"
     ]
    }
   ],
   "source": [
    "best_model = torch.load('./records/CNN_best_model_kernel_3_stride_1.pt')\n",
    "test_loss = test(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training 시각화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch_1\n",
       "0  0.000000\n",
       "1  0.000075"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./records/CNN_training_kernel_3_stride_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('AML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b32c03290ac83e85360a6b25c84a4d7d0173ff69cc4741b1b57cc3b22dc3e49c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
