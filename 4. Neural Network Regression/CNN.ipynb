{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "random_seed = 1234\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed_all(random_seed)  # if use multi-GPU\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "#torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)\n",
    "random.seed(random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "\n",
    "n_class = 63\n",
    "BATCH_SIZE = 128\n",
    "LR = 5e-3\n",
    "Train_epoch = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ = torch.load('data/Training/X_train_diet.pt') # torch.Size([16008, 240, 320, 1])\n",
    "X_test_ = torch.load('data/Testing/X_test_diet.pt') # torch.Size([1596, 240, 320, 1])\n",
    "\n",
    "Y_train_ = pd.read_csv('data/Training/Y_train.csv')\n",
    "Y_test_ = pd.read_csv('data/Testing/Y_test.csv')\n",
    "\n",
    "# dataframe to tensor\n",
    "Y_train_ = torch.tensor(Y_train_.values)  # torch.Size([16008, 63])\n",
    "Y_test_ = torch.tensor(Y_test_.values)  # torch.Size([1596, 63])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1596, 1, 240, 320])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# permute data\n",
    "\n",
    "X_train_ = X_train_.permute([0, 3, 1, 2])\n",
    "X_train_.shape\n",
    "\n",
    "X_test_ = X_test_.permute([0, 3, 1, 2])\n",
    "X_test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_and_split_dataloader(X_train_, X_test_, Y_train_, Y_test_, sample_size, valid_size, batch_size):\n",
    "\n",
    "    # sample train set\n",
    "    n_train = len(X_train_)\n",
    "    indices = list(range(n_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(sample_size * n_train))\n",
    "    sample_indices = indices[:split]\n",
    "    X_train_ = X_train_[sample_indices]\n",
    "    Y_train_ = Y_train_[sample_indices]\n",
    "\n",
    "    # # sample test set\n",
    "    # n_test = len(X_test_)\n",
    "    # indices = list(range(n_test))\n",
    "    # np.random.shuffle(indices)\n",
    "    # split = int(np.floor(sample_size * n_test))\n",
    "    # sample_indices = indices[:split]\n",
    "    # X_test_ = X_test_[sample_indices]\n",
    "    # Y_test_ = Y_test_[sample_indices]\n",
    "\n",
    "    # split train, valid set\n",
    "    n_train = len(X_train_)                  \n",
    "    indices = list(range(n_train))\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    split = int(np.floor(valid_size * n_train)) \n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    train_set = TensorDataset(X_train_, Y_train_)\n",
    "    train_sampler, valid_sampler = SubsetRandomSampler(train_idx), SubsetRandomSampler(valid_idx) \n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=train_sampler)\n",
    "    valid_loader = DataLoader(train_set, batch_size=batch_size, sampler=valid_sampler)\n",
    "\n",
    "    # test loader\n",
    "    test_set = TensorDataset(X_test_, Y_test_)\n",
    "    test_loader = DataLoader(test_set, batch_size=1)\n",
    "\n",
    "    # print shape\n",
    "    print('\\n Sample size: ', sample_size)\n",
    "    print('train_loader: ', len(train_loader)*train_loader.batch_size )\n",
    "    print('valid_loader: ', len(valid_loader)*valid_loader.batch_size )\n",
    "    print('test_loader: ', len(test_loader)*test_loader.batch_size )\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Sample size:  0.5\n",
      "train_loader:  6528\n",
      "valid_loader:  1664\n",
      "test_loader:  1596\n"
     ]
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader = sample_and_split_dataloader(\n",
    "    X_train_, X_test_, Y_train_, Y_test_, sample_size=0.5, valid_size=0.2, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_CNN(nn.Module): \n",
    "\n",
    "    def __init__(self, n_class, channels=[16, 32, 64]):\n",
    "        # super(my_CNN, self).__init__()\n",
    "        super().__init__()\n",
    "        # self.dropout - nn.Dropout(0.4)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, channels[0], kernel_size=5, stride=1, padding=2), # 240 * 320 * channels[0]\n",
    "            nn.BatchNorm2d(channels[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)) # 120 * 160 * channels[0]\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(channels[0], channels[1], kernel_size=5, stride=1, padding=2), # 120 * 160 * channels[1]\n",
    "            nn.BatchNorm2d(channels[1]),\n",
    "            nn.ReLU()\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2) # 60 * 80 * channels[1]\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(channels[1], channels[2], kernel_size=5, stride=1, padding=2), # 120 * 160 * channels[2]\n",
    "            nn.BatchNorm2d(channels[2]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 60 * 80 * channels[2]\n",
    "        ) \n",
    "        \n",
    "        self.fc = nn.Linear(60 * 80 * channels[2], n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(x.shape)  # torch.Size([1, 1, 240, 320])\n",
    "        out = self.layer1(x)  \n",
    "        # print(out.shape)  # torch.Size([1, 16, 120, 160])\n",
    "        out = self.layer2(out)\n",
    "        # print(out.shape)  # torch.Size([1, 32, 120, 160])\n",
    "        out = self.layer3(out)\n",
    "        # print(out.shape)  # torch.Size([1, 64, 60, 80])\n",
    "        out = out.reshape(out.size(0), -1) \n",
    "        # print(out.shape) # torch.Size([1, 307200])\n",
    "\n",
    "        out = self.fc(out) # error\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = my_CNN(63)\n",
    "# a(torch.randn(1, 1, 240, 320))  # B C H W 로 가짜 데이터 넣어보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \n",
    "    model = my_CNN(n_class).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = LR)\n",
    "    criterion = nn.MSELoss()\n",
    "    valid_loss_min = np.inf # 초기화 (나중에 업데이트 함)\n",
    "\n",
    "    for epoch in range(1, Train_epoch + 1): # epoch: 모든 데이터\n",
    "\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        for batch_id, (image, label) in enumerate(train_loader): # iter: batch 데이터 (25개) \n",
    "            label, image = label.to(device, dtype=torch.float), image.to(device, dtype=torch.float) # shape: (25,)\n",
    "            \n",
    "            output = model(image)   # 1. 모델에 데이터 입력해 출력 얻기 # 10개 클래스에 대한 로짓 # shape: (25, 10)\n",
    "            loss = criterion(output, label) # 2. loss 계산 \n",
    "            optimizer.zero_grad() # 3. 기울기 초기화 (iter 끝날때마다 초기화)\n",
    "            loss.backward() # 4. 역전파\n",
    "            optimizer.step() # 5. 최적화\n",
    "        \n",
    "        for batch_id, (image, label) in enumerate(valid_loader):\n",
    "            label, image = label.to(device, dtype=torch.float), image.to(device, dtype=torch.float)\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, label)\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "        # calculate avg losses\n",
    "        train_loss = train_loss/len(train_loader.dataset)\n",
    "        valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "        # save model if validation loss has decreased\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "            torch.save(model, 'best_model_CNN.pt')\n",
    "            torch.save(model.state_dict(), 'best_model_CNN.pth')\n",
    "            valid_loss_min = valid_loss\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "\n",
    "    model = torch.load('best_model_CNN.pt')  # 모델 불러오기\n",
    "    print('success load best_model')\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():  # 파라미터 업데이트 안 함\n",
    "\n",
    "        for batch_id, (image, label) in enumerate(test_loader):\n",
    "\n",
    "            label, image = label.to(device, dtype=torch.float), image.to(device, dtype=torch.float)\n",
    "            output = model(image)\n",
    "            loss = criterion(output, label)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    # calculate avg losses\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    \n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3913, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(105.9126, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(5331.8037, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1674.9882, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0409, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0402, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0379, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0381, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0384, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0403, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0396, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0388, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0393, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0390, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0385, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0389, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0390, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0371, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0395, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0411, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0385, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0416, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0398, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0414, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0404, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0391, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0413, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0391, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0406, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0381, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0385, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0419, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0393, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0394, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0387, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0400, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0392, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.0368, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "Epoch: 1 \tTraining Loss: 0.000000 \tValidation Loss: 0.000064\n",
      "Validation loss decreased (inf --> 0.000064).  Saving model ...\n",
      "success load best_model\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    model = train()\n",
    "    test_loss = test(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('AML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b32c03290ac83e85360a6b25c84a4d7d0173ff69cc4741b1b57cc3b22dc3e49c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
